{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_wordtokenizer(sentence):\n",
    "    words = WordPunctTokenizer().tokenize(sentence.lower())\n",
    "    return words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Search\n",
    "    Use Elastic Search to search from openbook.txt\n",
    "    Please make sure corpus has been put into Elastic Search using the index name \"corpus\". (es_create.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_sentences(sentence, sentence_size = 10, index_name = 'corpus', isParagraph = True):\n",
    "    search_body = {\n",
    "        \"query\": {\"match\":{\"text\": \n",
    "        {\n",
    "            \"query\": sentence\n",
    "        }}},\n",
    "        \"size\":  sentence_size\n",
    "    }\n",
    "    res_objs =es.search(index=index_name, body=search_body)['hits']['hits']\n",
    "    res = []\n",
    "    for obj in res_objs:\n",
    "        res.append(obj['_source']['text'][1:-1] + ' . ')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "    Calculate and build the word frequency dictionary from openbook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vocab = Counter()\n",
    "with codecs.open(config.corpus_path, 'r', 'utf-8') as fpr:\n",
    "    for line in fpr:\n",
    "        wordlist = en_wordtokenizer(line.strip())\n",
    "        for word in wordlist:\n",
    "            if word.lower() in frequency_vocab:\n",
    "                frequency_vocab[word.lower()] += 1\n",
    "            else:\n",
    "                frequency_vocab[word.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vocab_dict = {word[0]: ids for ids, word in enumerate(frequency_vocab.most_common())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set = []\n",
    "vocab = Counter()\n",
    "filenames = ['train', 'test', 'dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddWordToVocab(wordlist):\n",
    "    for word in wordlist:\n",
    "        if word.lower() in vocab:\n",
    "            vocab[word.lower()] += 1\n",
    "        else:\n",
    "            vocab[word.lower()] = 1\n",
    "\n",
    "def DeleteFrequentWord(sentence):\n",
    "    return [word for word in sentence if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadQuestion_json(data_path, isParagraph = True):\n",
    "    question_set = []\n",
    "    with codecs.open(data_path, 'r', 'utf-8') as data_f:\n",
    "        for line in data_f:\n",
    "            sample = json.loads(line.strip())\n",
    "            question = sample['question']['stem']\n",
    "            choices = sample['question']['choices']\n",
    "            question_text = en_wordtokenizer(question)\n",
    "            AddWordToVocab(question_text)\n",
    "            \n",
    "            q_c_cat = []\n",
    "            for choice in choices:\n",
    "                choice_text = en_wordtokenizer(choice['text'])\n",
    "                AddWordToVocab(choice_text)\n",
    "                \n",
    "                # ES\n",
    "                q_c = ' '.join(DeleteFrequentWord(choice_text + question_text))\n",
    "                q_c_es = es_sentences(q_c, 10, 'corpus', False)\n",
    "                for sen in q_c_es:\n",
    "                    AddWordToVocab(en_wordtokenizer(sen))\n",
    "                \n",
    "                q_c_cat.append({\"text\": choice['text'], \"es\": q_c_es})\n",
    "            answer = sample['answerKey']\n",
    "            question_set.append({'question': question, 'choices': q_c_cat, 'answer': answer})\n",
    "            \n",
    "    return question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ReadQuestion_json(config.train_json_path, False)\n",
    "test_set = ReadQuestion_json(config.test_json_path, False)\n",
    "dev_set = ReadQuestion_json(config.dev_json_path, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Vocab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_config import rel_projection\n",
    "for key, value in rel_projection.items():\n",
    "    AddWordToVocab(value)\n",
    "    \n",
    "vocab_list = ['<PAD>'] + sorted(vocab)\n",
    "vocab_size = len(vocab_list)\n",
    "vocab_dict = {word: i for i, word in enumerate(vocab_list) }\n",
    "\n",
    "vocab_name = 'data/vocab.txt'\n",
    "with codecs.open(vocab_name, 'w', 'utf-8') as fpw:\n",
    "    fpw.write('\\n'.join(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search ConceptNet \n",
    "    All the triplet of ConceptNet have been put in mysql (subject and object of the triplet should be english). Below is a example of the table for conceptnet.\n",
    "    \n",
    "```\n",
    "mysql> select * from en_graph where start = 'machine';\n",
    "+--------------+---------+-------------+\n",
    "| rel                       |      start     |        end            |\n",
    "+--------------+---------+-------------+\n",
    "| Antonym      | machine | human                           |\n",
    "| Antonym      | machine | organic                          |\n",
    "| Antonym      | machine | organic_things                |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_user = 'root'\n",
    "mysql_password = 'maxinyin'\n",
    "mysql_database = 'conceptnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from concept_config import rel_projection\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_most = {word[0]:ids for ids, word in enumerate(vocab.most_common())}\n",
    "def testValid(sentence):\n",
    "    for word in sentence:\n",
    "        if word not in vocab_list:\n",
    "            return None\n",
    "    return sentence\n",
    "\n",
    "def chooseSpecialWord(sentence, max_select = 3):\n",
    "    max_value = sorted(enumerate([vocab_most[word] for word in sentence]), key=lambda x:x[1], reverse=True)[0:min(max_select, len(sentence))]\n",
    "    return [sentence[ids] for ids, value in max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "db = pymysql.connect(\"localhost\", mysql_user, mysql_password, mysql_database)\n",
    "cursor = db.cursor()\n",
    "select_statement = \" \\\n",
    "    select distinct a.start, a.rel, a.end, b.rel, '{end_word}' \\\n",
    "    from   \\\n",
    "    ( \\\n",
    "        select start, end, rel from en_graph where start = '{start_word}' and end <> '{end_word}' \\\n",
    "    ) a  \\\n",
    "    left join en_graph b \\\n",
    "    on a.end = b.start  \\\n",
    "    where b.end like '%\\_{end_word}\\_%' or b.end = '{end_word}' or b.end like '{end_word}\\_%' or b.end like '%\\_{end_word}' \\\n",
    "          and a.start != a.end and b.start != b.end \\\n",
    "    union all \\\n",
    "    select start, rel, end, '', '' \\\n",
    "    from en_graph \\\n",
    "    where start = '{start_word}' and \\\n",
    "          (end like '%\\_{end_word}\\_%' or end = '{end_word}' or end like '{end_word}\\_%' or end like '%\\_{end_word}') ;\\\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_five_tuple(r, start_word, end_word):\n",
    "    start, rel, interstart, interrel, end = r\n",
    "    interstart = testValid(interstart.split('_'))\n",
    "    rel, interrel = rel_projection.get(rel, None), rel_projection.get(interrel, None)\n",
    "    if interstart == None:\n",
    "        return None\n",
    "    return {'start': start_word,\n",
    "              'rel': rel,\n",
    "              'interstart': interstart,\n",
    "              'interrel': interrel,\n",
    "              'end': end_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_concept(dataset):\n",
    "    start_time = time.time()\n",
    "    ids, total_sample = 0, len(dataset)\n",
    "    for sample in dataset:\n",
    "        ids += 1\n",
    "        if ids% 100 == 0:\n",
    "            print(\"Processing {}/{}\".format(ids, total_sample))\n",
    "        sample_question = chooseSpecialWord([word for word in en_wordtokenizer(sample['question']) if not word in stop_words])\n",
    "\n",
    "        for choice in sample['choices']:\n",
    "            sample_choice = chooseSpecialWord([word for word in en_wordtokenizer(choice['text']) if not word in stop_words])\n",
    "            q_c_set = [(q_word, c_word) for c_word in sample_choice for q_word in sample_question]\n",
    "            rel_count = 0\n",
    "            conceptnet_rel = []\n",
    "\n",
    "            for q_c in q_c_set:\n",
    "                res_1, res_2 = [], []\n",
    "                try:\n",
    "                    cursor.execute(select_statement.format(start_word= ps.stem(q_c[0]), end_word=ps.stem(q_c[1])))\n",
    "                    res_1 = cursor.fetchall()\n",
    "                    if len(res_1) == 0:\n",
    "                        cursor.execute(select_statement.format(start_word= ps.stem(q_c[1]), end_word=ps.stem(q_c[0])))\n",
    "                        res_2 = cursor.fetchall()\n",
    "                except:\n",
    "                    print('[ERROR]: ', q_c)\n",
    "\n",
    "                if len(res_1) > 0:\n",
    "                    conceptnet_rel.append(deal_five_tuple(res_1[0], q_c[0], q_c[1]))\n",
    "                elif len(res_2) > 0:\n",
    "                    conceptnet_rel.append(deal_five_tuple(res_2[0], q_c[1], q_c[0]))\n",
    "            choice['concept'] = conceptnet_rel\n",
    "    print(\"use_time: \",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR]:  (\"'\", 'entire')\n",
      "[ERROR]:  (\"'\", 'smooth')\n",
      "[ERROR]:  (\"'\", 'surface')\n",
      "[ERROR]:  (\"'\", 'cavities')\n",
      "[ERROR]:  (\"'\", 'explosions')\n",
      "[ERROR]:  (\"'\", 'large')\n",
      "[ERROR]:  (\"'\", 'core')\n",
      "[ERROR]:  (\"'\", 'cheese')\n",
      "[ERROR]:  (\"'\", 'internal')\n",
      "[ERROR]:  (\"'\", 'filled')\n",
      "[ERROR]:  (\"'\", 'lakes')\n",
      "[ERROR]:  ('flow', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('conductor', \"'\")\n",
      "[ERROR]:  ('flow', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('conductor', \"'\")\n",
      "[ERROR]:  ('flow', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('conductor', \"'\")\n",
      "[ERROR]:  (\"'\", 'tranquilitiy')\n",
      "[ERROR]:  (\"'\", 'sea')\n",
      "[ERROR]:  (\"'\", 'caspian')\n",
      "[ERROR]:  (\"'\", 'sea')\n",
      "[ERROR]:  (\"'\", 'sea')\n",
      "[ERROR]:  (\"'\", 'dead')\n",
      "[ERROR]:  (\"'\", 'caribbean')\n",
      "[ERROR]:  (\"'\", 'sea')\n",
      "Processing 100/4957\n",
      "Processing 200/4957\n",
      "[ERROR]:  ('chopped', \"'\")\n",
      "[ERROR]:  ('apple', \"'\")\n",
      "[ERROR]:  ('place', \"'\")\n",
      "[ERROR]:  ('caused', \"'\")\n",
      "[ERROR]:  ('seasons', \"'\")\n",
      "Processing 300/4957\n",
      "[ERROR]:  ('duck', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('melting', \"'\")\n",
      "[ERROR]:  ('duck', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('melting', \"'\")\n",
      "[ERROR]:  (\"'\", 'marks')\n",
      "[ERROR]:  (\"'\", 'asteroid')\n",
      "[ERROR]:  (\"'\", 'impact')\n",
      "[ERROR]:  (\"'\", 'smooth')\n",
      "[ERROR]:  (\"'\", 'filled')\n",
      "[ERROR]:  (\"'\", 'lakes')\n",
      "[ERROR]:  (\"'\", 'gravitational')\n",
      "[ERROR]:  (\"'\", 'stronger')\n",
      "[ERROR]:  (\"'\", 'pull')\n",
      "Processing 400/4957\n",
      "[ERROR]:  (\"'\", 'nutrition')\n",
      "[ERROR]:  (\"'\", 'biology')\n",
      "[ERROR]:  (\"'\", 'composition')\n",
      "[ERROR]:  (\"'\", 'chemical')\n",
      "[ERROR]:  (\"'\", 'formation')\n",
      "Processing 500/4957\n",
      "[ERROR]:  ('following', \"'\")\n",
      "[ERROR]:  ('zero', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('home', \"'\")\n",
      "[ERROR]:  ('cut', \"'\")\n",
      "[ERROR]:  ('forest', \"'\")\n",
      "[ERROR]:  ('seen', \"'\")\n",
      "[ERROR]:  ('inside', \"'\")\n",
      "[ERROR]:  ('often', \"'\")\n",
      "Processing 600/4957\n",
      "[ERROR]:  (\"'\", 'conduit')\n",
      "[ERROR]:  (\"'\", 'acts')\n",
      "[ERROR]:  (\"'\", 'lungs')\n",
      "[ERROR]:  (\"'\", 'command')\n",
      "[ERROR]:  (\"'\", 'base')\n",
      "[ERROR]:  (\"'\", 'acts')\n",
      "[ERROR]:  (\"'\", 'reasoning')\n",
      "[ERROR]:  (\"'\", 'spacial')\n",
      "[ERROR]:  (\"'\", 'lacks')\n",
      "[ERROR]:  (\"'\", 'assists')\n",
      "[ERROR]:  (\"'\", 'movements')\n",
      "[ERROR]:  ('sails', \"'\")\n",
      "[ERROR]:  ('ship', \"'\")\n",
      "[ERROR]:  ('house', \"'\")\n",
      "Processing 700/4957\n",
      "[ERROR]:  ('crust', \"'\")\n",
      "[ERROR]:  ('layer', \"'\")\n",
      "[ERROR]:  ('found', \"'\")\n",
      "[ERROR]:  ('indoors', \"'\")\n",
      "[ERROR]:  ('well', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('determine', \"'\")\n",
      "[ERROR]:  ('punnett', \"'\")\n",
      "[ERROR]:  ('square', \"'\")\n",
      "Processing 800/4957\n",
      "Processing 900/4957\n",
      "[ERROR]:  (\"'\", 'plug')\n",
      "[ERROR]:  (\"'\", 'nose')\n",
      "[ERROR]:  (\"'\", 'mouth')\n",
      "[ERROR]:  (\"'\", 'tape')\n",
      "[ERROR]:  (\"'\", 'plugs')\n",
      "[ERROR]:  (\"'\", 'ear')\n",
      "[ERROR]:  (\"'\", 'hands')\n",
      "[ERROR]:  (\"'\", 'back')\n",
      "[ERROR]:  (\"'\", 'behind')\n",
      "Processing 1000/4957\n",
      "[ERROR]:  ('classroom', \"'\")\n",
      "[ERROR]:  ('laptop', \"'\")\n",
      "[ERROR]:  ('laptop', \"'\")\n",
      "[ERROR]:  (\"'\", 'seasons')\n",
      "[ERROR]:  (\"'\", 'tides')\n",
      "[ERROR]:  (\"'\", 'sunset')\n",
      "[ERROR]:  (\"'\", 'clocks')\n",
      "Processing 1100/4957\n",
      "[ERROR]:  (\"'\", 'cozy')\n",
      "[ERROR]:  (\"'\", 'enables')\n",
      "[ERROR]:  (\"'\", 'keep')\n",
      "[ERROR]:  (\"'\", 'enables')\n",
      "[ERROR]:  (\"'\", 'eat')\n",
      "[ERROR]:  (\"'\", 'burden')\n",
      "[ERROR]:  (\"'\", 'useless')\n",
      "[ERROR]:  (\"'\", 'health')\n",
      "[ERROR]:  (\"'\", 'difficult')\n",
      "[ERROR]:  (\"'\", 'function')\n",
      "[ERROR]:  (\"'\", 'bear')\n",
      "Processing 1200/4957\n",
      "Processing 1300/4957\n",
      "Processing 1400/4957\n",
      "[ERROR]:  (\"'\", 'jupiter')\n",
      "[ERROR]:  (\"'\", 'moon')\n",
      "[ERROR]:  (\"'\", 'dewarf')\n",
      "[ERROR]:  (\"'\", 'yellow')\n",
      "[ERROR]:  (\"'\", 'titan')\n",
      "[ERROR]:  ('slamming', \"'\")\n",
      "[ERROR]:  ('breaks', \"'\")\n",
      "[ERROR]:  ('car', \"'\")\n",
      "[ERROR]:  ('goose', \"'\")\n",
      "[ERROR]:  ('map', \"'\")\n",
      "[ERROR]:  ('heads', \"'\")\n",
      "[ERROR]:  (\"'\", 'landscape')\n",
      "[ERROR]:  (\"'\", 'features')\n",
      "[ERROR]:  (\"'\", 'features')\n",
      "[ERROR]:  (\"'\", 'completely')\n",
      "[ERROR]:  (\"'\", 'covered')\n",
      "[ERROR]:  (\"'\", 'water')\n",
      "[ERROR]:  (\"'\", '%')\n",
      "[ERROR]:  (\"'\", 'flat')\n",
      "[ERROR]:  (\"'\", 'smooth')\n",
      "[ERROR]:  (\"'\", 'asteroid')\n",
      "[ERROR]:  (\"'\", 'created')\n",
      "[ERROR]:  (\"'\", '%')\n",
      "[ERROR]:  ('diverse', \"'\")\n",
      "[ERROR]:  ('marine', \"'\")\n",
      "[ERROR]:  ('observe', \"'\")\n",
      "Processing 1500/4957\n",
      "[ERROR]:  ('uv', \"'\")\n",
      "[ERROR]:  ('due', \"'\")\n",
      "[ERROR]:  ('rays', \"'\")\n",
      "Processing 1600/4957\n",
      "[ERROR]:  ('feed', \"'\")\n",
      "[ERROR]:  ('needs', \"'\")\n",
      "[ERROR]:  ('plant', \"'\")\n",
      "Processing 1700/4957\n",
      "[ERROR]:  ('attributed', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('attributed', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('attributed', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('attributed', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "[ERROR]:  ('equator', \"'\")\n",
      "Processing 1800/4957\n",
      "[ERROR]:  ('vancouver', \"'\")\n",
      "[ERROR]:  ('gliding', \"'\")\n",
      "[ERROR]:  ('due', \"'\")\n",
      "Processing 1900/4957\n",
      "[ERROR]:  ('torso', \"'\")\n",
      "[ERROR]:  ('perspiration', \"'\")\n",
      "[ERROR]:  ('leaving', \"'\")\n",
      "[ERROR]:  (\"'\", 'lay')\n",
      "[ERROR]:  (\"'\", 'eggs')\n",
      "[ERROR]:  (\"'\", 'give')\n",
      "[ERROR]:  (\"'\", 'birth')\n",
      "[ERROR]:  (\"'\", 'live')\n",
      "[ERROR]:  (\"'\", 'reframe')\n",
      "[ERROR]:  (\"'\", 'reproduction')\n",
      "[ERROR]:  (\"'\", 'aid')\n",
      "[ERROR]:  (\"'\", 'give')\n",
      "[ERROR]:  (\"'\", 'live')\n",
      "Processing 2000/4957\n",
      "[ERROR]:  ('lake', \"'\")\n",
      "[ERROR]:  ('much', \"'\")\n",
      "[ERROR]:  ('receives', \"'\")\n",
      "Processing 2100/4957\n",
      "[ERROR]:  ('farmer', \"'\")\n",
      "[ERROR]:  ('experiencing', \"'\")\n",
      "[ERROR]:  ('pebbles', \"'\")\n",
      "[ERROR]:  ('dehydration', \"'\")\n",
      "[ERROR]:  ('forgot', \"'\")\n",
      "[ERROR]:  ('bring', \"'\")\n",
      "[ERROR]:  ('talc', \"'\")\n",
      "[ERROR]:  ('talc', \"'\")\n",
      "[ERROR]:  ('able', \"'\")\n",
      "Processing 2200/4957\n",
      "[ERROR]:  ('thin', \"'\")\n",
      "[ERROR]:  ('dog', \"'\")\n",
      "[ERROR]:  ('ears', \"'\")\n",
      "[ERROR]:  ('thin', \"'\")\n",
      "[ERROR]:  ('dog', \"'\")\n",
      "[ERROR]:  ('ears', \"'\")\n",
      "[ERROR]:  ('thin', \"'\")\n",
      "[ERROR]:  ('dog', \"'\")\n",
      "[ERROR]:  ('ears', \"'\")\n",
      "[ERROR]:  ('thin', \"'\")\n",
      "[ERROR]:  ('dog', \"'\")\n",
      "[ERROR]:  ('ears', \"'\")\n",
      "[ERROR]:  ('geese', \"'\")\n",
      "[ERROR]:  ('know', \"'\")\n",
      "[ERROR]:  ('way', \"'\")\n",
      "[ERROR]:  ('1440', \"'\")\n",
      "[ERROR]:  ('minutes', \"'\")\n",
      "[ERROR]:  ('every', \"'\")\n",
      "[ERROR]:  ('involving', \"'\")\n",
      "[ERROR]:  ('results', \"'\")\n",
      "[ERROR]:  ('baking', \"'\")\n",
      "[ERROR]:  ('considered', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('learned', \"'\")\n",
      "Processing 2300/4957\n",
      "[ERROR]:  ('block', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('destination', \"'\")\n",
      "[ERROR]:  ('reach', \"'\")\n",
      "[ERROR]:  ('following', \"'\")\n",
      "Processing 2400/4957\n",
      "[ERROR]:  ('depends', \"'\")\n",
      "[ERROR]:  ('currently', \"'\")\n",
      "[ERROR]:  ('upon', \"'\")\n",
      "[ERROR]:  ('depends', \"'\")\n",
      "[ERROR]:  ('currently', \"'\")\n",
      "[ERROR]:  ('upon', \"'\")\n",
      "[ERROR]:  ('depends', \"'\")\n",
      "[ERROR]:  ('currently', \"'\")\n",
      "[ERROR]:  ('upon', \"'\")\n",
      "[ERROR]:  ('depends', \"'\")\n",
      "[ERROR]:  ('currently', \"'\")\n",
      "[ERROR]:  ('upon', \"'\")\n",
      "Processing 2500/4957\n",
      "[ERROR]:  ('oak', \"'\")\n",
      "[ERROR]:  ('leaf', \"'\")\n",
      "[ERROR]:  ('falls', \"'\")\n",
      "[ERROR]:  ('bigfoot', \"'\")\n",
      "[ERROR]:  ('suzy', \"'\")\n",
      "[ERROR]:  ('blame', \"'\")\n",
      "[ERROR]:  ('bigfoot', \"'\")\n",
      "[ERROR]:  ('suzy', \"'\")\n",
      "[ERROR]:  ('blame', \"'\")\n",
      "[ERROR]:  ('tulip', \"'\")\n",
      "[ERROR]:  ('red', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('tulip', \"'\")\n",
      "[ERROR]:  ('red', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('tulip', \"'\")\n",
      "[ERROR]:  ('red', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('testifies', \"'\")\n",
      "[ERROR]:  ('scandinavian', \"'\")\n",
      "[ERROR]:  ('eye', \"'\")\n",
      "[ERROR]:  ('least', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('find', \"'\")\n",
      "[ERROR]:  ('least', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('find', \"'\")\n",
      "[ERROR]:  ('least', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('find', \"'\")\n",
      "[ERROR]:  ('least', \"'\")\n",
      "[ERROR]:  ('would', \"'\")\n",
      "[ERROR]:  ('find', \"'\")\n",
      "Processing 2600/4957\n",
      "Processing 2700/4957\n",
      "[ERROR]:  ('experiment', \"'\")\n",
      "[ERROR]:  ('right', \"'\")\n",
      "[ERROR]:  ('lab', \"'\")\n",
      "[ERROR]:  (\"'\", 'happiness')\n",
      "[ERROR]:  (\"'\", 'love')\n",
      "[ERROR]:  (\"'\", 'anger')\n",
      "[ERROR]:  (\"'\", 'creation')\n",
      "[ERROR]:  (\"'\", 'magnetic')\n",
      "Processing 2800/4957\n",
      "[ERROR]:  ('learned', \"'\")\n",
      "[ERROR]:  ('behavior', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('learned', \"'\")\n",
      "[ERROR]:  ('behavior', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "Processing 2900/4957\n",
      "[ERROR]:  ('10pm', \"'\")\n",
      "[ERROR]:  ('2am', \"'\")\n",
      "[ERROR]:  ('dipper', \"'\")\n",
      "Processing 3000/4957\n",
      "Processing 3100/4957\n",
      "Processing 3200/4957\n",
      "Processing 3300/4957\n",
      "[ERROR]:  ('ensure', \"'\")\n",
      "[ERROR]:  ('free', \"'\")\n",
      "[ERROR]:  ('stay', \"'\")\n",
      "[ERROR]:  ('molecules', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('likely', \"'\")\n",
      "[ERROR]:  ('effect', \"'\")\n",
      "[ERROR]:  ('acid', \"'\")\n",
      "[ERROR]:  ('could', \"'\")\n",
      "[ERROR]:  ('following', \"'\")\n",
      "[ERROR]:  ('uses', \"'\")\n",
      "[ERROR]:  ('fuel', \"'\")\n",
      "[ERROR]:  ('brown', \"'\")\n",
      "[ERROR]:  ('hour', \"'\")\n",
      "[ERROR]:  ('later', \"'\")\n",
      "Processing 3400/4957\n",
      "[ERROR]:  (\"'\", 'formation')\n",
      "[ERROR]:  (\"'\", 'biology')\n",
      "[ERROR]:  (\"'\", 'composition')\n",
      "[ERROR]:  (\"'\", 'chemical')\n",
      "[ERROR]:  (\"'\", 'nutrition')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3500/4957\n",
      "Processing 3600/4957\n",
      "[ERROR]:  ('remote', \"'\")\n",
      "[ERROR]:  ('farm', \"'\")\n",
      "[ERROR]:  ('fridge', \"'\")\n",
      "[ERROR]:  ('remote', \"'\")\n",
      "[ERROR]:  ('farm', \"'\")\n",
      "[ERROR]:  ('fridge', \"'\")\n",
      "[ERROR]:  ('remote', \"'\")\n",
      "[ERROR]:  ('farm', \"'\")\n",
      "[ERROR]:  ('fridge', \"'\")\n",
      "[ERROR]:  ('wasted', \"'\")\n",
      "[ERROR]:  ('stare', \"'\")\n",
      "[ERROR]:  ('rotations', \"'\")\n",
      "[ERROR]:  ('18th', \"'\")\n",
      "[ERROR]:  ('century', \"'\")\n",
      "[ERROR]:  ('crossing', \"'\")\n",
      "[ERROR]:  ('permanent', \"'\")\n",
      "[ERROR]:  ('fixed', \"'\")\n",
      "[ERROR]:  ('due', \"'\")\n",
      "[ERROR]:  ('way', \"'\")\n",
      "[ERROR]:  ('always', \"'\")\n",
      "[ERROR]:  ('make', \"'\")\n",
      "Processing 3700/4957\n",
      "[ERROR]:  ('celestial', \"'\")\n",
      "[ERROR]:  ('mass', \"'\")\n",
      "[ERROR]:  ('body', \"'\")\n",
      "[ERROR]:  ('guavas', \"'\")\n",
      "[ERROR]:  ('leaves', \"'\")\n",
      "[ERROR]:  ('many', \"'\")\n",
      "Processing 3800/4957\n",
      "[ERROR]:  ('harbor', \"'\")\n",
      "[ERROR]:  ('pearl', \"'\")\n",
      "[ERROR]:  ('japan', \"'\")\n",
      "[ERROR]:  ('stranded', \"'\")\n",
      "[ERROR]:  ('potentially', \"'\")\n",
      "[ERROR]:  ('sahara', \"'\")\n",
      "[ERROR]:  ('gull', \"'\")\n",
      "[ERROR]:  ('build', \"'\")\n",
      "[ERROR]:  ('nest', \"'\")\n",
      "Processing 3900/4957\n",
      "[ERROR]:  (\"'\", 'sustenance')\n",
      "[ERROR]:  (\"'\", 'wet')\n",
      "[ERROR]:  (\"'\", 'trees')\n",
      "[ERROR]:  (\"'\", 'clouds')\n",
      "[ERROR]:  (\"'\", 'shoes')\n",
      "[ERROR]:  ('tides', \"'\")\n",
      "[ERROR]:  ('locations', \"'\")\n",
      "[ERROR]:  ('different', \"'\")\n",
      "Processing 4000/4957\n",
      "[ERROR]:  ('doorknob', \"'\")\n",
      "[ERROR]:  ('provides', \"'\")\n",
      "[ERROR]:  ('open', \"'\")\n",
      "[ERROR]:  ('watering', \"'\")\n",
      "[ERROR]:  ('lots', \"'\")\n",
      "[ERROR]:  ('better', \"'\")\n",
      "Processing 4100/4957\n",
      "Processing 4200/4957\n",
      "[ERROR]:  ('watch', \"'\")\n",
      "[ERROR]:  ('wants', \"'\")\n",
      "[ERROR]:  ('eclipse', \"'\")\n",
      "[ERROR]:  (\"'\", 'nourishment')\n",
      "[ERROR]:  (\"'\", 'plays')\n",
      "[ERROR]:  (\"'\", 'rhythm')\n",
      "[ERROR]:  (\"'\", 'heartbeat')\n",
      "[ERROR]:  ('mostly', \"'\")\n",
      "[ERROR]:  ('colors', \"'\")\n",
      "[ERROR]:  ('closest', \"'\")\n",
      "[ERROR]:  ('mostly', \"'\")\n",
      "[ERROR]:  ('colors', \"'\")\n",
      "[ERROR]:  ('closest', \"'\")\n",
      "[ERROR]:  ('completes', \"'\")\n",
      "[ERROR]:  ('circuit', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('completes', \"'\")\n",
      "[ERROR]:  ('circuit', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('completes', \"'\")\n",
      "[ERROR]:  ('circuit', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "Processing 4300/4957\n",
      "[ERROR]:  ('photosynthesize', \"'\")\n",
      "[ERROR]:  ('cell', \"'\")\n",
      "[ERROR]:  ('photosynthesize', \"'\")\n",
      "[ERROR]:  ('cell', \"'\")\n",
      "[ERROR]:  ('photosynthesize', \"'\")\n",
      "[ERROR]:  ('cell', \"'\")\n",
      "[ERROR]:  ('photosynthesize', \"'\")\n",
      "[ERROR]:  ('cell', \"'\")\n",
      "[ERROR]:  ('much', \"'\")\n",
      "[ERROR]:  ('iron', \"'\")\n",
      "Processing 4400/4957\n",
      "[ERROR]:  (\"'\", 'flying')\n",
      "[ERROR]:  (\"'\", 'money')\n",
      "[ERROR]:  (\"'\", 'restaurants')\n",
      "[ERROR]:  (\"'\", 'sustenance')\n",
      "Processing 4500/4957\n",
      "[ERROR]:  ('iron', \"'\")\n",
      "[ERROR]:  ('find', \"'\")\n",
      "[ERROR]:  ('number', \"'\")\n",
      "[ERROR]:  ('human', \"'\")\n",
      "[ERROR]:  ('characteristic', \"'\")\n",
      "Processing 4600/4957\n",
      "[ERROR]:  ('smallpox', \"'\")\n",
      "[ERROR]:  ('children', \"'\")\n",
      "[ERROR]:  ('get', \"'\")\n",
      "[ERROR]:  ('excrete', \"'\")\n",
      "[ERROR]:  ('octopuses', \"'\")\n",
      "[ERROR]:  ('ink', \"'\")\n",
      "Processing 4700/4957\n",
      "[ERROR]:  ('swing', \"'\")\n",
      "[ERROR]:  ('kid', \"'\")\n",
      "[ERROR]:  ('easy', \"'\")\n",
      "[ERROR]:  (\"'\", 'pet')\n",
      "[ERROR]:  (\"'\", 'dog')\n",
      "[ERROR]:  (\"'\", 'cuddle')\n",
      "[ERROR]:  (\"'\", 'dog')\n",
      "[ERROR]:  (\"'\", 'love')\n",
      "[ERROR]:  (\"'\", 'dog')\n",
      "[ERROR]:  (\"'\", 'stomp')\n",
      "[ERROR]:  (\"'\", 'puppy')\n",
      "Processing 4800/4957\n",
      "[ERROR]:  ('self', \"'\")\n",
      "[ERROR]:  ('love', \"'\")\n",
      "[ERROR]:  ('absorbed', \"'\")\n",
      "[ERROR]:  ('impaired', \"'\")\n",
      "[ERROR]:  ('function', \"'\")\n",
      "[ERROR]:  ('lungs', \"'\")\n",
      "[ERROR]:  (\"'\", 'seaside')\n",
      "[ERROR]:  (\"'\", 'coast')\n",
      "[ERROR]:  (\"'\", 'peak')\n",
      "[ERROR]:  (\"'\", 'mountain')\n",
      "[ERROR]:  (\"'\", 'high')\n",
      "[ERROR]:  (\"'\", 'island')\n",
      "[ERROR]:  (\"'\", 'ocean')\n",
      "[ERROR]:  (\"'\", 'beach')\n",
      "[ERROR]:  (\"'\", 'sunny')\n",
      "Processing 4900/4957\n",
      "[ERROR]:  ('easiest', \"'\")\n",
      "[ERROR]:  ('rose', \"'\")\n",
      "[ERROR]:  ('grow', \"'\")\n",
      "[ERROR]:  ('walks', \"'\")\n",
      "[ERROR]:  ('go', \"'\")\n",
      "[ERROR]:  ('time', \"'\")\n",
      "[ERROR]:  ('walks', \"'\")\n",
      "[ERROR]:  ('go', \"'\")\n",
      "[ERROR]:  ('time', \"'\")\n",
      "[ERROR]:  ('walks', \"'\")\n",
      "[ERROR]:  ('go', \"'\")\n",
      "[ERROR]:  ('time', \"'\")\n",
      "[ERROR]:  ('without', \"'\")\n",
      "[ERROR]:  ('survive', \"'\")\n",
      "[ERROR]:  ('help', \"'\")\n",
      "use_time:  4687.8161833286285\n",
      "[ERROR]:  ('influence', \"'\")\n",
      "[ERROR]:  ('beak', \"'\")\n",
      "[ERROR]:  ('ability', \"'\")\n",
      "Processing 100/500\n",
      "[ERROR]:  ('perennial', \"'\")\n",
      "[ERROR]:  ('elongated', \"'\")\n",
      "[ERROR]:  ('lumber', \"'\")\n",
      "[ERROR]:  ('conducts', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('electricity', \"'\")\n",
      "Processing 200/500\n",
      "[ERROR]:  ('occurrence', \"'\")\n",
      "[ERROR]:  ('activity', \"'\")\n",
      "[ERROR]:  ('nuclear', \"'\")\n",
      "[ERROR]:  (\"'\", 'fireball')\n",
      "[ERROR]:  (\"'\", 'celestial')\n",
      "[ERROR]:  (\"'\", 'flares')\n",
      "[ERROR]:  (\"'\", 'solar')\n",
      "[ERROR]:  (\"'\", 'gamma')\n",
      "[ERROR]:  (\"'\", 'rays')\n",
      "[ERROR]:  (\"'\", 'bang')\n",
      "[ERROR]:  (\"'\", 'big')\n",
      "Processing 300/500\n",
      "[ERROR]:  ('trash', \"'\")\n",
      "[ERROR]:  ('cellular', \"'\")\n",
      "[ERROR]:  ('respiration', \"'\")\n",
      "[ERROR]:  ('trash', \"'\")\n",
      "[ERROR]:  ('cellular', \"'\")\n",
      "[ERROR]:  ('respiration', \"'\")\n",
      "[ERROR]:  ('trash', \"'\")\n",
      "[ERROR]:  ('cellular', \"'\")\n",
      "[ERROR]:  ('respiration', \"'\")\n",
      "[ERROR]:  ('trash', \"'\")\n",
      "[ERROR]:  ('cellular', \"'\")\n",
      "[ERROR]:  ('respiration', \"'\")\n",
      "[ERROR]:  ('know', \"'\")\n",
      "[ERROR]:  ('going', \"'\")\n",
      "[ERROR]:  ('fast', \"'\")\n",
      "Processing 400/500\n",
      "[ERROR]:  (\"'\", 'distance')\n",
      "[ERROR]:  (\"'\", 'shape')\n",
      "[ERROR]:  (\"'\", 'size')\n",
      "[ERROR]:  (\"'\", 'temperature')\n",
      "[ERROR]:  (\"'\", 'rip')\n",
      "[ERROR]:  (\"'\", 'corner')\n",
      "[ERROR]:  (\"'\", 'portion')\n",
      "[ERROR]:  (\"'\", 'table')\n",
      "[ERROR]:  (\"'\", 'flat')\n",
      "[ERROR]:  (\"'\", 'lay')\n",
      "[ERROR]:  (\"'\", 'edges')\n",
      "[ERROR]:  (\"'\", 'color')\n",
      "[ERROR]:  (\"'\", 'add')\n",
      "[ERROR]:  (\"'\", 'piece')\n",
      "[ERROR]:  (\"'\", 'tape')\n",
      "[ERROR]:  ('chipmunk', \"'\")\n",
      "[ERROR]:  ('acorns', \"'\")\n",
      "Processing 500/500\n",
      "use_time:  441.6399314403534\n",
      "[ERROR]:  ('revolving', \"'\")\n",
      "[ERROR]:  ('around', \"'\")\n",
      "[ERROR]:  ('sun', \"'\")\n",
      "[ERROR]:  ('peat', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('found', \"'\")\n",
      "[ERROR]:  ('peat', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('found', \"'\")\n",
      "[ERROR]:  ('peat', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('found', \"'\")\n",
      "[ERROR]:  ('peat', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('found', \"'\")\n",
      "[ERROR]:  ('retain', \"'\")\n",
      "[ERROR]:  ('owl', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('retain', \"'\")\n",
      "[ERROR]:  ('owl', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('retain', \"'\")\n",
      "[ERROR]:  ('owl', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "[ERROR]:  ('retain', \"'\")\n",
      "[ERROR]:  ('owl', \"'\")\n",
      "[ERROR]:  ('?', \"'\")\n",
      "Processing 100/500\n",
      "[ERROR]:  ('sitting', \"'\")\n",
      "[ERROR]:  ('left', \"'\")\n",
      "[ERROR]:  ('nest', \"'\")\n",
      "Processing 200/500\n",
      "[ERROR]:  (\"'\", 'orbit')\n",
      "[ERROR]:  (\"'\", 'pulls')\n",
      "[ERROR]:  (\"'\", 'stars')\n",
      "[ERROR]:  (\"'\", 'ebb')\n",
      "[ERROR]:  (\"'\", 'neap')\n",
      "[ERROR]:  (\"'\", 'determines')\n",
      "[ERROR]:  (\"'\", 'grounded')\n",
      "[ERROR]:  (\"'\", 'keeps')\n",
      "[ERROR]:  (\"'\", 'objects')\n",
      "[ERROR]:  (\"'\", 'forces')\n",
      "[ERROR]:  (\"'\", 'orbit')\n",
      "[ERROR]:  (\"'\", 'around')\n",
      "Processing 300/500\n",
      "[ERROR]:  ('contributes', \"'\")\n",
      "[ERROR]:  ('able', \"'\")\n",
      "[ERROR]:  ('mate', \"'\")\n",
      "[ERROR]:  (\"'\", 'supplies')\n",
      "[ERROR]:  (\"'\", 'roots')\n",
      "[ERROR]:  (\"'\", 'nutrients')\n",
      "[ERROR]:  (\"'\", 'root')\n",
      "[ERROR]:  (\"'\", 'supplies')\n",
      "[ERROR]:  (\"'\", 'dioxide')\n",
      "[ERROR]:  (\"'\", 'acts')\n",
      "[ERROR]:  (\"'\", 'lungs')\n",
      "[ERROR]:  (\"'\", 'plant')\n",
      "[ERROR]:  (\"'\", 'delivering')\n",
      "[ERROR]:  (\"'\", 'subway')\n",
      "[ERROR]:  (\"'\", 'passengers')\n",
      "Processing 400/500\n",
      "[ERROR]:  ('phase', \"'\")\n",
      "[ERROR]:  ('change', \"'\")\n",
      "Processing 500/500\n",
      "use_time:  486.4965817928314\n"
     ]
    }
   ],
   "source": [
    "get_concept(train_set)\n",
    "get_concept(test_set)\n",
    "get_concept(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_index(dataset, filename): \n",
    "    print('preprocessing ' + filename + '...')\n",
    "    fpw = open(os.path.join('data/', filename+'_info.tsv'), 'w')\n",
    "    writer=csv.writer(fpw, delimiter='\\t')\n",
    "    for sample in dataset:\n",
    "        row = []\n",
    "        \n",
    "        #question =  [str(vocab_dict[word.lower()]) for word in en_wordtokenizer(sample['question'])]\n",
    "        question =  [word.lower() for word in en_wordtokenizer(sample['question'])]\n",
    "        row.append(' '.join(question))\n",
    "        choices = sample['choices']\n",
    "        \n",
    "        for choice in choices:\n",
    "            #text = question + [str(vocab_dict[word.lower()]) for word in en_wordtokenizer(choice['text'])]\n",
    "            text = [word.lower() for word in en_wordtokenizer(choice['text'])]\n",
    "            #concept_text = [str(word_index[word.lower()]) for word in en_wordtokenizer(choice['concept'])]\n",
    "            row.append(' '.join(text))\n",
    "            \n",
    "            # concept\n",
    "            concepts_count = 0\n",
    "            for concept in choice.get('concept', []):\n",
    "                if concept == None:\n",
    "                    continue\n",
    "                if concept['interrel'] == None:\n",
    "                    concept_sentence = concept['start'] + ' , ' + ' '.join(concept['rel']) + ' , ' + ' '.join(concept['interstart']) + ' .'\n",
    "                else:\n",
    "                    concept_sentence = concept['start'] + ' , ' + ' '.join(concept['rel']) + ' , ' + ' '.join(concept['interstart']) + \\\n",
    "                                   ' , ' + ' '.join(concept['interrel']) + ' , ' + concept['end'] + ' .'\n",
    "                try:\n",
    "                    #concept_single = [str(vocab_dict[word.lower()]) for word in en_wordtokenizer(concept_sentence)]\n",
    "                    concept_single = [word.lower() for word in en_wordtokenizer(concept_sentence)]\n",
    "                except:\n",
    "                    print(choice)\n",
    "                    fpw.close()\n",
    "                    return \n",
    "                row.append(' '.join(concept_single))\n",
    "                concepts_count += 1\n",
    "            for _ in range(9-concepts_count):\n",
    "                row.append('.')\n",
    "            \n",
    "            # es\n",
    "            ess = choice.get('es', [])\n",
    "            for es in ess:\n",
    "                row.append(es)\n",
    "            for _ in range(10-len(ess)):\n",
    "                row.append('.')\n",
    "                \n",
    "        label = str(sample['answer'])\n",
    "        row.append(label)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    fpw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dev...\n",
      "preprocessing train...\n",
      "preprocessing test...\n"
     ]
    }
   ],
   "source": [
    "write_index(dev_set, 'dev')\n",
    "write_index(train_set, 'train')\n",
    "write_index(test_set, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def find_in_ordered_list_binary(word):\n",
    "    start_id = 0\n",
    "    end_id = vocab_size - 1\n",
    "    while start_id <= end_id:\n",
    "        mid_id = (start_id + end_id) // 2\n",
    "        if vocab_list[mid_id] == word:\n",
    "            return True\n",
    "        elif word > vocab_list[mid_id]:\n",
    "            start_id = mid_id + 1\n",
    "        else:\n",
    "            end_id = mid_id - 1\n",
    "    return False\n",
    "    \n",
    "def GenerateSmallGlove(glove_path):\n",
    "    glove_embed = {}\n",
    "    glove_vocab = []\n",
    "    \n",
    "    with codecs.open(glove_path, 'r', 'utf-8') as glove_f:\n",
    "        for line in glove_f:\n",
    "            row = line.strip().split(' ')\n",
    "            if find_in_ordered_list_binary(row[0]):\n",
    "                glove_embed[row[0]] = row[1:]\n",
    "                glove_vocab.append(row[0])\n",
    "    print(\"Glove find {} words\".format(len(glove_vocab)))\n",
    "    return glove_embed, glove_vocab\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "glove_word_embed, glove_word_vocab = GenerateSmallGlove('/data/mxy/nlp/glove.840B.300d.txt')\n",
    "remaining_word = set(vocab_list) - set(glove_word_vocab)\n",
    "print(\"{} words have no embedding\".format(len(remaining_word)))\n",
    "glove_remain_word = glove_word_vocab + list(remaining_word)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "glove_filename = 'glove.300d.small'\n",
    "glove_file = open(os.path.join('data/', glove_filename + '_concept.txt'), 'w')\n",
    "zero_embd = ['0'] * 300\n",
    "glove_file.write('<pad> ' + ' '.join(zero_embd) + '\\n') \n",
    "for word in vocab_list[1:]:\n",
    "    glove_file.write(word + ' ') \n",
    "    str_embd = [str(index) for index in glove_word_embed.get(word, zero_embd)]\n",
    "    glove_file.write(' '.join(str_embd) + '\\n')\n",
    "\n",
    "glove_file.close()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
